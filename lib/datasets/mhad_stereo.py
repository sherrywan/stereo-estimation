'''
Author: sherrywaan sherrywaan@outlook.com
Date: 2023-03-14 21:32:03
LastEditors: sherrywaan sherrywaan@outlook.com
LastEditTime: 2023-09-09 13:52:34
FilePath: /wxy/3d_pose/stereo-estimation/lib/dataset/mhad_stereo.py
Description: stereo view dataset of  MHAD_Berkeley
'''
import os
from collections import defaultdict
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset

from lib.utils.camera import Camera
from lib.utils import multiview
from lib.utils.img import resize_image, crop_image, normalize_image, scale_bbox, crop_keypoints_img, resize_keypoints_img


class MHADStereoViewDataset(Dataset):
    """
        MHAD_Berkeley for stereoview tasks
    """
    def __init__(
            self,
            mhad_root='/data1/share/dataset/MHAD_Berkeley/stereo_camera',
            labels_path='/data1/share/dataset/MHAD_Berkeley/stereo_camera/extra/human36m-stereo-s-labels-GTbboxes.npy',
            pred_results_path=None,
            image_shape=(256, 256),
            heatmap_shape=(96, 96),
            train=False,
            test=False,
            retain_every_n_frames_in_test=1,
            cuboid_side=2000.0,
            scale_bbox=1.5,
            norm_image=True,
            kind="mpii",
            crop=True,
            action=None,
            dataset="mhad",
            rectificated=True,
            baseline_width='s'):
        """
            mhad_root:
                Path to 'stereo_camera/' directory in MHAD
            labels_path:
                Path to 'human36m-stereo-s-labels-GTbboxes.npy' generated by 'generate-stereo-labels.py'
            retain_every_n_frames_in_test:
            kind:
                Keypoint format, 'mpii' or 'mhad'
            dataset:
                Dataset, "mhad"
            rectificated:
                If the stereo images are rectificated
            baseline_width:
                Type of baseline width of stereo cameras
        """
        assert train or test, '`MHADStereoViewDataset` must be constructed with at least ' \
                              'one of `test=True` / `train=True`'
        assert kind in ("mpii", "mhad")
        self.joints_name = {
            0 : 'RFoot',
            1 : 'RKnee',
            2 : 'RHip',
            3 : 'LFoot',
            4 : 'LKnee',
            5 : 'LHip',
            6 : 'Spine',
            7 : 'Thorax',
            8 : 'Neck',
            9 : 'Head',
            10 : 'RShoulder',
            11 : 'RElbow',
            12 : 'RWrist',
            13 : 'LShoulder',
            14 : 'LElbow',
            15 : 'LWrist',
            16 : 'Nose'
        }
        self.mhad_root = mhad_root
        self.labels_path = labels_path
        self.image_shape = None if image_shape is None else tuple(image_shape)
        self.heatmape_shape = None if heatmap_shape is None else tuple(
            heatmap_shape)
        self.scale_bbox = scale_bbox
        self.norm_image = norm_image
        self.cuboid_side = cuboid_side
        self.kind = kind
        self.crop = crop
        self.action_target = action
        self.dataset = dataset
        self.rectificated = rectificated
        self.baseline_width = baseline_width

        self.labels = np.load(labels_path, allow_pickle=True).item()
        
        train_subjects = ['S01','S02','S03','S04','S05','S06','S07','S09','S10','S12']
        test_subjects = ['S08', 'S11']
        train_subjects = list(self.labels['subject_names'].index(x)
                                for x in train_subjects)
        test_subjects = list(self.labels['subject_names'].index(x)
                                for x in test_subjects)

        indices = []
        if train:
            mask = np.isin(self.labels['table']['subject_idx'],
                           train_subjects,
                           assume_unique=True)
            indices.append(np.nonzero(mask)[0])
        if test:
            mask = np.isin(self.labels['table']['subject_idx'],
                           test_subjects,
                           assume_unique=True)
            indices.append(np.nonzero(mask)[0][::retain_every_n_frames_in_test])

        self.labels['table'] = self.labels['table'][np.concatenate(indices)]

        self.num_keypoints = 16 if kind == "mpii" else 17

        self.keypoints_3d_pred = None
        if pred_results_path is not None:
            pred_results = np.load(pred_results_path, allow_pickle=True)
            keypoints_3d_pred = pred_results['keypoints_3d'][np.argsort(
                pred_results['indexes'])]
            self.keypoints_3d_pred = keypoints_3d_pred[::
                                                       retain_every_n_frames_in_test]
            assert len(self.keypoints_3d_pred) == len(self), \
                f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
                f"has {len(self.keypoints_3d_pred)}. Did you follow all preprocessing instructions carefully?"


    def __len__(self):
        return len(self.labels['table'])


    def __getitem__(self, idx):
        sample = defaultdict(list)  # return value
        shot = self.labels['table'][idx]

        group_idx = shot['group_idx']
        subject_idx = shot['subject_idx']
        action_idx = shot['action_idx']
        reptition_idx = shot['reptition_idx']
        subject = self.labels['subject_names'][subject_idx]
        action = self.labels['action_names'][action_idx]
        reptition = self.labels['reptition_names'][reptition_idx]
        frame_idx = shot['frame_idx']
        
        if self.action_target is not None and self.action_target not in action:
            return None
        
        keypoints_3d = np.pad(shot['keypoints'][:self.num_keypoints],
                                        ((0, 0), (0, 1)),
                                        'constant',
                                        constant_values=1.0)
        for camera_idx, camera_name in enumerate(self.labels['camera_names'][group_idx]):
            # load bounding box
            bbox = shot['bbox_by_camera_tlbr'][camera_idx][[1, 0, 3, 2]]  # TLBR to LTRB
            bbox_height = bbox[2] - bbox[0]
            if bbox_height == 0:
                # convention: if the bbox is empty, then this view is missing
                continue

            # scale the bounding box
            bbox = scale_bbox(bbox, self.scale_bbox)

            # load image
            image_path = os.path.join(
                self.mhad_root, 'Cluster01', 'rectificated' * self.rectificated, 
                self.baseline_width * self.rectificated, str(group_idx) * self.rectificated,
                'Cam%02d'%(int(camera_name)), subject, action, reptition, 
                'img_l01_c%02d_s%02d_a%02d_r%02d_%05d.jpg' % (int(camera_name), subject_idx+1, action_idx+1, reptition_idx+1, frame_idx))
            
            assert os.path.isfile(image_path), '%s doesn\'t exist' % image_path
            image = cv2.imread(image_path)

            # load camera
            shot_camera = self.labels['cameras'][group_idx][camera_idx]
            T = -shot_camera['R'].T @ shot_camera['t']
            retval_camera = Camera(shot_camera['R'], T,
                                   shot_camera['t'], shot_camera['K'],
                                   name = camera_name)
            
            # load keypoints_2d
            keypoints_2d = shot['keypoints_2d'][:self.num_keypoints][camera_idx]

            if self.crop:
                # crop image
                image = crop_image(image, bbox)
                retval_camera.update_after_crop(bbox)
                keypoints_2d = crop_keypoints_img(keypoints_2d, bbox)

            if self.image_shape is not None:
                # resize
                image_shape_before_resize = image.shape[:2]
                image = resize_image(image, self.image_shape)
                retval_camera.update_after_resize(image_shape_before_resize,
                                                  self.image_shape)
                sample['image_shapes_before_resize'].append(
                    image_shape_before_resize)
                keypoints_2d = resize_keypoints_img(keypoints_2d, image_shape_before_resize, self.image_shape)

            if self.norm_image:
                image = normalize_image(image, type="MHAD")
            
            # 3d keypoints in camera coordiate
            keypoints_3d_ca = retval_camera.extrinsics @ keypoints_3d[:,:,np.newaxis]

            sample['images'].append(image)
            sample['detections'].append(bbox +
                                        (1.0, ))  # TODO add real confidences
            sample['cameras'].append(retval_camera)
            sample['proj_matrices'].append(retval_camera.projection)
            sample['keypoints_3d_ca'].append(keypoints_3d_ca)
            
        sample['keypoints_2d'] = shot['keypoints_2d'][:self.num_keypoints]
        sample['keypoints_3d'] = keypoints_3d
        sample['indexes'] = idx
        sample['occlusion'] = shot['occlusion_2d'][:, :self.num_keypoints]

        if self.keypoints_3d_pred is not None:
            sample['pred_keypoints_3d'] = self.keypoints_3d_pred[idx]

        sample.default_factory = None

        return sample


    def evaluate_using_per_pose_error(self, per_pose_error, split_by_subject):
        def evaluate_by_actions(self, per_pose_error, mask=None):
            if mask is None:
                mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {
                    'total_loss': per_pose_error[mask].sum(),
                    'frame_count': np.count_nonzero(mask)
                }
            }

            for action_idx in range(len(self.labels['action_names'])):
                action_mask = (self.labels['table']['action_idx']
                               == action_idx) & mask
                action_per_pose_error = per_pose_error[action_mask]
                action_scores[self.labels['action_names'][action_idx]] = {
                    'total_loss': action_per_pose_error.sum(),
                    'frame_count': len(action_per_pose_error)
                }

            for k, v in action_scores.items():
                action_scores[k] = float('nan') if v['frame_count'] == 0 else (
                    v['total_loss'] / v['frame_count'])

            return action_scores

        subject_scores = {'Average': evaluate_by_actions(self, per_pose_error)}

        for subject_idx in range(len(self.labels['subject_names'])):
            subject_mask = self.labels['table']['subject_idx'] == subject_idx
            subject_scores[self.labels['subject_names'][subject_idx]] = \
                evaluate_by_actions(self, per_pose_error, subject_mask)

        return subject_scores


    def evaluate(self,
                 keypoints_3d_predicted,
                 split_by_subject=False):
        '''
        occluded: if the keypoints are occluded, 0 for not occluded, 1 for oclluded, 2 for all
        '''
        occlusion = self.labels['table']['occlusion_2d'][:, :self.num_keypoints]
        keypoints_gt = self.labels['table']['keypoints'][:, :self.num_keypoints]
        root_index = 0
        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:,root_index:root_index+1, :]
        
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError('`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))
        
        # not occluded keypoints
        not_occluded = np.nonzero(occlusion==0)
        keypoints_gt_notocc = np.expand_dims(keypoints_gt[not_occluded[0], not_occluded[1],:], axis=0)
        keypoints_3d_predicted_notocc = np.expand_dims(keypoints_3d_predicted[not_occluded[0], not_occluded[1], :], axis=0)
        keypoints_gt_relative_notocc = np.expand_dims(keypoints_gt_relative[not_occluded[0], not_occluded[1], :], axis=0)
        keypoints_3d_predicted_relative_notocc = np.expand_dims(keypoints_3d_predicted_relative[not_occluded[0], not_occluded[1], :], axis=0)
        
        # occluded keypoints
        occluded = np.nonzero(occlusion==1)
        keypoints_gt_occ = np.expand_dims(keypoints_gt[occluded[0], occluded[1],:], axis=0)
        keypoints_3d_predicted_occ = np.expand_dims(keypoints_3d_predicted[occluded[0], occluded[1], :], axis=0)
        keypoints_gt_relative_occ = np.expand_dims(keypoints_gt_relative[occluded[0], occluded[1], :], axis=0)
        keypoints_3d_predicted_relative_occ = np.expand_dims(keypoints_3d_predicted_relative[occluded[0], occluded[1], :], axis=0)
        
        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(1)  
        per_joint_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted)**2).sum(2)).mean(0)
        per_pose_error_notocc = np.sqrt(((keypoints_gt_notocc - keypoints_3d_predicted_notocc)**2).sum(2)).mean(1)
        per_pose_error_occ = np.sqrt(((keypoints_gt_occ - keypoints_3d_predicted_occ)**2).sum(2)).mean(1)
        per_pose_error_relative_notocc = np.sqrt(((keypoints_gt_relative_notocc - keypoints_3d_predicted_relative_notocc)**2).sum(2)).mean(1)
        per_pose_error_relative_occ = np.sqrt(((keypoints_gt_relative_occ - keypoints_3d_predicted_relative_occ)**2).sum(2)).mean(1)
       
        # relative mean error per 16/17 joints in mm, for each pose
        per_pose_error_relative = np.sqrt(((keypoints_gt_relative - keypoints_3d_predicted_relative)**2).sum(2)).mean(1)
        
        # bone length mean error per 8/16 bone in mm for each pose
        skeleton_idx = [[6, 3], [6, 2], [2, 1], [3, 4], [1, 0], [4, 5], [6, 7],
                        [7, 8], [8, 9], [8, 12], [8, 13], [12, 11], [11, 10],
                        [13, 14], [14, 15]]
        trans_matric = np.zeros((len(skeleton_idx), self.num_keypoints))
        # KCS矩阵
        for ske_idx, ske in enumerate(skeleton_idx):
            trans_matric[ske_idx, ske[0]] = 1
            trans_matric[ske_idx, ske[1]] = -1
        bone_gt = np.matmul(trans_matric[np.newaxis, :, :], keypoints_gt)
        bone_pred = np.matmul(trans_matric[np.newaxis, :, :], keypoints_3d_predicted)

        per_pose_bonelength_error = abs(np.sqrt((bone_gt**2).sum(2)) - np.sqrt((bone_pred**2).sum(2))).mean(1)

        result = {
            'per_pose_bonelength_error':
            self.evaluate_using_per_pose_error(per_pose_bonelength_error,
                                               split_by_subject),
            'per_pose_error':
            self.evaluate_using_per_pose_error(per_pose_error,
                                               split_by_subject),
            'per_pose_error_relative':
            self.evaluate_using_per_pose_error(per_pose_error_relative,
                                               split_by_subject),
            'per_joint_error':
            {'Average': per_joint_error.tolist()},
            'per_error_notocc':
            {'Average': per_pose_error_notocc.tolist()},
            'per_error_occ':
            {'Average': per_pose_error_occ.tolist()},
            'per_error_relative_notocc':
            {'Average': per_pose_error_relative_notocc.tolist()},
            'per_error_relative_occ':
            {'Average': per_pose_error_relative_occ.tolist()}
        }

        return result['per_pose_error_relative']['Average'][
            'Average'], result['per_pose_error']['Average'][
            'Average'], result
    
    def JDR_2d(self, keypoints_2d_pred, keypoints_2d_gt):
        pred = keypoints_2d_pred.copy().reshape(-1,17,2)
        gt = keypoints_2d_gt.copy().reshape(-1,17,2)

        headsize = self.image_shape[0] / 10.0
        threshold = 0.5

        distance = np.sqrt(np.sum((gt - pred)**2, axis=2))
        detected = (distance <= headsize * threshold)

        joint_detection_rate = np.sum(detected, axis=0) / np.float(gt.shape[0])

        name_values = {}
        joint_names = self.joints_name
        for i in range(len(joint_names)):
            name_values[joint_names[i]] = joint_detection_rate[i]
        name_values["avg"] = np.mean(joint_detection_rate)
        return name_values, np.mean(joint_detection_rate)